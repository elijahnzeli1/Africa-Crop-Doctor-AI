{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycAgBbWe_HbZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AFRICAN CROP DOCTOR - COMPLETE CLASSIFICATION SYSTEM\n",
    "A production-ready system for crop disease diagnosis using classification\n",
    "\n",
    "Total Parameters: ~35M (Vision: 2.5M + Optional Language: 22M)\n",
    "Training Time: 7-10 days on CPU\n",
    "Inference Speed: <300ms on mobile phone\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KAGGLE DATASET DOWNLOAD & SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENHANCED MULTI-DATASET DOWNLOAD & FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_fuse_datasets():\n",
    "    \"\"\"\n",
    "    Download and fuse multiple crop disease datasets for comprehensive training\n",
    "    Combines PlantDoc + 20K Multi-class + PlantDisease datasets\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ ENHANCED MULTI-DATASET PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Download all datasets\n",
    "    datasets = {\n",
    "        'plantdoc': kagglehub.dataset_download(\"abdulhasibuddin/plant-doc-dataset\"),\n",
    "        'multiclass_20k': kagglehub.dataset_download(\"jawadali1045/20k-multi-class-crop-disease-images\"),\n",
    "        'plantdisease': kagglehub.dataset_download(\"emmarex/plantdisease\")\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… All datasets downloaded successfully!\")\n",
    "    for name, path in datasets.items():\n",
    "        print(f\"   ğŸ“ {name}: {path}\")\n",
    "    \n",
    "    # Organize unified data structure\n",
    "    base_dir = Path(\"unified_crop_data\")\n",
    "    if base_dir.exists():\n",
    "        shutil.rmtree(base_dir)\n",
    "    \n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    (base_dir / \"train\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (base_dir / \"val\" / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Comprehensive crop and disease mappings\n",
    "    all_image_data = []\n",
    "    disease_names = set()\n",
    "    crop_names = set()\n",
    "    \n",
    "    print(\"\\nğŸ” Processing datasets...\")\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, dataset_path in datasets.items():\n",
    "        print(f\"\\nğŸ“Š Processing {dataset_name}...\")\n",
    "        dataset_path = Path(dataset_path)\n",
    "        \n",
    "        # Find all image files\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n",
    "        image_files = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(dataset_path.rglob(f\"*{ext}\")))\n",
    "        \n",
    "        print(f\"   Found {len(image_files)} images\")\n",
    "        \n",
    "        # Process images and extract labels\n",
    "        for img_path in image_files:\n",
    "            if img_path.is_file():\n",
    "                # Extract info from path and filename\n",
    "                path_parts = str(img_path).lower().split(os.sep)\n",
    "                filename = img_path.stem.lower()\n",
    "                \n",
    "                # Advanced label extraction\n",
    "                disease_info, crop_info = extract_labels_advanced(path_parts, filename)\n",
    "                \n",
    "                if disease_info and crop_info:\n",
    "                    all_image_data.append({\n",
    "                        'path': img_path,\n",
    "                        'disease': disease_info,\n",
    "                        'crop': crop_info,\n",
    "                        'source': dataset_name\n",
    "                    })\n",
    "                    disease_names.add(disease_info)\n",
    "                    crop_names.add(crop_info)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ UNIFIED DATASET STATISTICS:\")\n",
    "    print(f\"   ğŸ“ˆ Total images: {len(all_image_data)}\")\n",
    "    print(f\"   ğŸ¦  Unique diseases: {len(disease_names)}\")\n",
    "    print(f\"   ğŸŒ¾ Unique crops: {len(crop_names)}\")\n",
    "    \n",
    "    # Create comprehensive mappings\n",
    "    disease_to_id = {disease: idx for idx, disease in enumerate(sorted(disease_names))}\n",
    "    crop_to_id = {crop: idx for idx, crop in enumerate(sorted(crop_names))}\n",
    "    \n",
    "    # Enhanced data balancing and augmentation\n",
    "    balanced_data = balance_dataset(all_image_data)\n",
    "    \n",
    "    # Split data (80% train, 20% val)\n",
    "    np.random.shuffle(balanced_data)\n",
    "    split_idx = int(0.8 * len(balanced_data))\n",
    "    \n",
    "    train_data = balanced_data[:split_idx]\n",
    "    val_data = balanced_data[split_idx:]\n",
    "    \n",
    "    # Organize and save files\n",
    "    train_labels = {}\n",
    "    val_labels = {}\n",
    "    \n",
    "    print(f\"\\nğŸ“ Organizing {len(train_data)} training samples...\")\n",
    "    for i, item in enumerate(train_data):\n",
    "        new_name = f\"train_{i:06d}.jpg\"\n",
    "        dst_path = base_dir / \"train\" / \"images\" / new_name\n",
    "        \n",
    "        # Copy and resize image for consistency\n",
    "        copy_and_resize_image(item['path'], dst_path)\n",
    "        \n",
    "        train_labels[new_name] = {\n",
    "            'disease': disease_to_id[item['disease']],\n",
    "            'crop': crop_to_id[item['crop']],\n",
    "            'severity': get_severity_from_disease(item['disease']),\n",
    "            'source': item['source']\n",
    "        }\n",
    "    \n",
    "    print(f\"ğŸ“ Organizing {len(val_data)} validation samples...\")\n",
    "    for i, item in enumerate(val_data):\n",
    "        new_name = f\"val_{i:06d}.jpg\"\n",
    "        dst_path = base_dir / \"val\" / \"images\" / new_name\n",
    "        \n",
    "        copy_and_resize_image(item['path'], dst_path)\n",
    "        \n",
    "        val_labels[new_name] = {\n",
    "            'disease': disease_to_id[item['disease']],\n",
    "            'crop': crop_to_id[item['crop']],\n",
    "            'severity': get_severity_from_disease(item['disease']),\n",
    "            'source': item['source']\n",
    "        }\n",
    "    \n",
    "    # Save label files\n",
    "    with open(base_dir / \"train\" / \"labels.json\", 'w') as f:\n",
    "        json.dump(train_labels, f, indent=2)\n",
    "    \n",
    "    with open(base_dir / \"val\" / \"labels.json\", 'w') as f:\n",
    "        json.dump(val_labels, f, indent=2)\n",
    "    \n",
    "    # Save comprehensive mappings\n",
    "    mappings = {\n",
    "        'disease_to_id': disease_to_id,\n",
    "        'crop_to_id': crop_to_id,\n",
    "        'id_to_disease': {v: k for k, v in disease_to_id.items()},\n",
    "        'id_to_crop': {v: k for k, v in crop_to_id.items()},\n",
    "        'dataset_stats': {\n",
    "            'total_samples': len(balanced_data),\n",
    "            'train_samples': len(train_data),\n",
    "            'val_samples': len(val_data),\n",
    "            'num_diseases': len(disease_names),\n",
    "            'num_crops': len(crop_names),\n",
    "            'source_distribution': get_source_distribution(balanced_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(base_dir / \"mappings.json\", 'w') as f:\n",
    "        json.dump(mappings, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… UNIFIED DATASET CREATED!\")\n",
    "    print(f\"   ğŸ“ˆ Training: {len(train_labels)} images\")\n",
    "    print(f\"   ğŸ“Š Validation: {len(val_labels)} images\")\n",
    "    print(f\"   ğŸ¦  Diseases: {len(disease_names)}\")\n",
    "    print(f\"   ğŸŒ¾ Crops: {len(crop_names)}\")\n",
    "    \n",
    "    return base_dir, len(disease_names), len(crop_names)\n",
    "\n",
    "\n",
    "def extract_labels_advanced(path_parts, filename):\n",
    "    \"\"\"Advanced label extraction from multiple dataset formats\"\"\"\n",
    "    \n",
    "    # Comprehensive crop patterns\n",
    "    crops_mapping = {\n",
    "        'apple': 'apple', 'corn': 'corn', 'maize': 'corn', 'grape': 'grape', \n",
    "        'tomato': 'tomato', 'potato': 'potato', 'pepper': 'pepper', 'bell_pepper': 'pepper',\n",
    "        'strawberry': 'strawberry', 'peach': 'peach', 'cherry': 'cherry', \n",
    "        'soybean': 'soybean', 'rice': 'rice', 'wheat': 'wheat', 'cassava': 'cassava',\n",
    "        'banana': 'banana', 'coffee': 'coffee', 'cotton': 'cotton', 'sugarcane': 'sugarcane',\n",
    "        'orange': 'orange', 'lemon': 'lemon', 'mango': 'mango', 'coconut': 'coconut'\n",
    "    }\n",
    "    \n",
    "    # Disease patterns\n",
    "    disease_patterns = [\n",
    "        'blight', 'rust', 'spot', 'rot', 'mildew', 'wilt', 'mosaic', 'streak', \n",
    "        'scab', 'canker', 'anthracnose', 'bacterial', 'viral', 'fungal',\n",
    "        'healthy', 'powdery', 'downy', 'leaf_spot', 'early_blight', 'late_blight'\n",
    "    ]\n",
    "    \n",
    "    # Combine path and filename for analysis\n",
    "    full_text = ' '.join(path_parts + [filename]).lower()\n",
    "    \n",
    "    # Extract crop\n",
    "    crop_info = None\n",
    "    for crop_key, crop_value in crops_mapping.items():\n",
    "        if crop_key in full_text:\n",
    "            crop_info = crop_value\n",
    "            break\n",
    "    \n",
    "    # Extract disease\n",
    "    disease_info = None\n",
    "    if 'healthy' in full_text:\n",
    "        disease_info = 'healthy'\n",
    "    else:\n",
    "        for disease in disease_patterns:\n",
    "            if disease in full_text:\n",
    "                # Create more specific disease name\n",
    "                if crop_info:\n",
    "                    disease_info = f\"{crop_info}_{disease}\"\n",
    "                else:\n",
    "                    disease_info = disease\n",
    "                break\n",
    "        \n",
    "        # Fallback for generic diseased samples\n",
    "        if not disease_info and crop_info:\n",
    "            disease_info = f\"{crop_info}_disease\"\n",
    "    \n",
    "    return disease_info, crop_info\n",
    "\n",
    "\n",
    "def balance_dataset(image_data):\n",
    "    \"\"\"Balance dataset to ensure good representation of all classes\"\"\"\n",
    "    \n",
    "    # Group by disease and crop\n",
    "    disease_groups = defaultdict(list)\n",
    "    crop_groups = defaultdict(list)\n",
    "    \n",
    "    for item in image_data:\n",
    "        disease_groups[item['disease']].append(item)\n",
    "        crop_groups[item['crop']].append(item)\n",
    "    \n",
    "    # Calculate target samples per class\n",
    "    min_disease_samples = max(50, min(len(samples) for samples in disease_groups.values()))\n",
    "    \n",
    "    # Balance by undersampling overrepresented classes and duplicating underrepresented\n",
    "    balanced_data = []\n",
    "    \n",
    "    for samples in disease_groups.values():\n",
    "        if len(samples) > min_disease_samples * 3:\n",
    "            # Undersample\n",
    "            sampled = np.random.choice(samples, min_disease_samples * 2, replace=False)\n",
    "            balanced_data.extend(sampled)\n",
    "        elif len(samples) < min_disease_samples:\n",
    "            # Oversample by duplication\n",
    "            multiplier = max(2, min_disease_samples // len(samples))\n",
    "            balanced_data.extend(samples * multiplier)\n",
    "        else:\n",
    "            balanced_data.extend(samples)\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "\n",
    "def copy_and_resize_image(src_path, dst_path, target_size=256):\n",
    "    \"\"\"Copy and resize image for consistency\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(src_path))\n",
    "        if img is not None:\n",
    "            # Resize while maintaining aspect ratio\n",
    "            h, w = img.shape[:2]\n",
    "            if h > w:\n",
    "                new_h, new_w = target_size, int(w * target_size / h)\n",
    "            else:\n",
    "                new_h, new_w = int(h * target_size / w), target_size\n",
    "            \n",
    "            img_resized = cv2.resize(img, (new_w, new_h))\n",
    "            \n",
    "            # Pad to square\n",
    "            pad_h = (target_size - new_h) // 2\n",
    "            pad_w = (target_size - new_w) // 2\n",
    "            \n",
    "            img_padded = cv2.copyMakeBorder(\n",
    "                img_resized, pad_h, target_size - new_h - pad_h,\n",
    "                pad_w, target_size - new_w - pad_w,\n",
    "                cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "            )\n",
    "            \n",
    "            cv2.imwrite(str(dst_path), img_padded)\n",
    "        else:\n",
    "            # Fallback: just copy original\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "    except:\n",
    "        # Fallback: just copy original\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "\n",
    "def get_severity_from_disease(disease_name):\n",
    "    \"\"\"Estimate severity from disease name\"\"\"\n",
    "    if 'healthy' in disease_name.lower():\n",
    "        return 0  # healthy\n",
    "    elif any(word in disease_name.lower() for word in ['mild', 'early', 'slight']):\n",
    "        return 1  # mild\n",
    "    elif any(word in disease_name.lower() for word in ['late', 'severe', 'advanced']):\n",
    "        return 3  # severe\n",
    "    else:\n",
    "        return 2  # moderate\n",
    "\n",
    "\n",
    "def get_source_distribution(data):\n",
    "    \"\"\"Get distribution of samples by source dataset\"\"\"\n",
    "    dist = defaultdict(int)\n",
    "    for item in data:\n",
    "        dist[item['source']] += 1\n",
    "    return dict(dist)\n",
    "\n",
    "\n",
    "# Execute enhanced dataset preparation\n",
    "print(\"ğŸš€ STARTING ENHANCED MULTI-DATASET PREPARATION...\")\n",
    "enhanced_data_dir, enhanced_num_diseases, enhanced_num_crops = download_and_fuse_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_nq55CRAA2h"
   },
   "source": [
    "#### PART 1: VISION CLASSIFIER (Core Disease Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENHANCED VISION CLASSIFIER (Improved Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDiseaseClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced classifier with improved architecture and training stability\n",
    "    Features: Attention mechanism, Better regularization, Progressive learning\n",
    "    \"\"\"\n",
    "    def __init__(self, num_diseases=50, num_crops=15):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_diseases = num_diseases\n",
    "        self.num_crops = num_crops\n",
    "        \n",
    "        # Use EfficientNet-B0 (better than MobileNet for accuracy)\n",
    "        self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Get feature dimension by doing a forward pass\n",
    "        # This is the most reliable way for any model architecture\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            # Temporarily remove classifier to get feature size\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            features = self.backbone(dummy_input)\n",
    "            in_features = features.shape[1]\n",
    "            print(f\"ğŸ” Detected feature dimensions: {in_features}\")\n",
    "        \n",
    "        # Feature enhancement layer\n",
    "        self.feature_enhancer = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for better feature selection\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Disease classification head (with more capacity)\n",
    "        self.disease_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_diseases)\n",
    "        )\n",
    "        \n",
    "        # Crop classification head\n",
    "        self.crop_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_crops)\n",
    "        )\n",
    "        \n",
    "        # Severity estimation head\n",
    "        self.severity_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Proper weight initialization for better training\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Enhance features\n",
    "        enhanced_features = self.feature_enhancer(features)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(enhanced_features)\n",
    "        attended_features = enhanced_features * attention_weights\n",
    "        \n",
    "        # Multi-task outputs\n",
    "        disease_logits = self.disease_head(attended_features)\n",
    "        crop_logits = self.crop_head(attended_features)\n",
    "        severity_logits = self.severity_head(attended_features)\n",
    "        \n",
    "        return {\n",
    "            'disease': disease_logits,\n",
    "            'crop': crop_logits,\n",
    "            'severity': severity_logits,\n",
    "            'features': attended_features  # For analysis\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2fsp1wfAO8o"
   },
   "source": [
    "#### PART 2: DATA PREPROCESSING & AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBzRf1qU_eYS"
   },
   "outputs": [],
   "source": [
    "class CropDiseaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader with African farm-specific augmentations\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, labels_file, training=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.labels = self._load_labels(labels_file)\n",
    "        self.training = training\n",
    "\n",
    "        # African farm-specific augmentations\n",
    "        if training:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                # Simulate different lighting conditions in Africa\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.3,  # Strong sun vs shade\n",
    "                    contrast=0.3,\n",
    "                    saturation=0.2,\n",
    "                    hue=0.1\n",
    "                ),\n",
    "                transforms.RandomRotation(30),\n",
    "                # Simulate dust/blur from phone cameras\n",
    "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    def _load_labels(self, labels_file):\n",
    "        # Format: {\"image_name.jpg\": {\"disease\": 5, \"crop\": 2, \"severity\": 1}}\n",
    "        with open(labels_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = list(self.labels.keys())[idx]\n",
    "        image_path = self.image_dir / image_name\n",
    "\n",
    "        # Read image\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get labels\n",
    "        labels = self.labels[image_name]\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'disease': labels['disease'],\n",
    "            'crop': labels['crop'],\n",
    "            'severity': labels['severity']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2tGX7cxAa2Y"
   },
   "source": [
    "#### PART 3: TRAINING PIPELINE (CPU-Optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED TRAINING PIPELINE (Smart Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCropDoctorTrainer:\n",
    "    \"\"\"\n",
    "    Advanced training pipeline with smart learning strategies\n",
    "    Features: Curriculum learning, Advanced scheduling, Better loss weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Training metrics storage\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_disease_accuracies = []\n",
    "        self.val_crop_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.epoch_times = []\n",
    "        self.loss_components = {'disease': [], 'crop': [], 'severity': []}\n",
    "        \n",
    "        # Advanced loss functions with class weighting\n",
    "        self.setup_loss_functions()\n",
    "        \n",
    "        # Smart optimizer with warmup\n",
    "        self.setup_optimizer()\n",
    "        \n",
    "        # Advanced scheduler\n",
    "        self.setup_scheduler()\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_disease_acc = 0.0\n",
    "        self.best_overall_acc = 0.0\n",
    "        self.patience = 10\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def setup_loss_functions(self):\n",
    "        \"\"\"Setup weighted loss functions for better class balance\"\"\"\n",
    "        # You might want to calculate actual class weights from your dataset\n",
    "        self.disease_criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.crop_criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "        self.severity_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def setup_optimizer(self):\n",
    "        \"\"\"Setup optimizer with different learning rates for different parts\"\"\"\n",
    "        # Different learning rates for backbone vs heads\n",
    "        backbone_params = list(self.model.backbone.parameters())\n",
    "        head_params = list(self.model.disease_head.parameters()) + \\\n",
    "                     list(self.model.crop_head.parameters()) + \\\n",
    "                     list(self.model.severity_head.parameters()) + \\\n",
    "                     list(self.model.feature_enhancer.parameters()) + \\\n",
    "                     list(self.model.attention.parameters())\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 1e-4, 'weight_decay': 0.01},  # Lower LR for pretrained\n",
    "            {'params': head_params, 'lr': 1e-3, 'weight_decay': 0.01}       # Higher LR for new layers\n",
    "        ])\n",
    "    \n",
    "    def setup_scheduler(self):\n",
    "        \"\"\"Advanced learning rate scheduling\"\"\"\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=[1e-3, 2e-3],  # Different max LR for each param group\n",
    "            epochs=50,\n",
    "            steps_per_epoch=len(self.train_loader),\n",
    "            pct_start=0.3,  # Warmup for 30% of training\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "    \n",
    "    def compute_loss(self, outputs, labels, epoch):\n",
    "        \"\"\"\n",
    "        Compute weighted multi-task loss with curriculum learning\n",
    "        \"\"\"\n",
    "        disease_labels = labels['disease']\n",
    "        crop_labels = labels['crop']\n",
    "        severity_labels = labels['severity']\n",
    "        \n",
    "        # Individual losses\n",
    "        disease_loss = self.disease_criterion(outputs['disease'], disease_labels)\n",
    "        crop_loss = self.crop_criterion(outputs['crop'], crop_labels)\n",
    "        severity_loss = self.severity_criterion(outputs['severity'], severity_labels)\n",
    "        \n",
    "        # Progressive loss weighting (curriculum learning)\n",
    "        # Start with easier tasks (crop) then gradually focus on harder tasks (disease)\n",
    "        progress = min(epoch / 20.0, 1.0)  # 20 epochs for full progression\n",
    "        \n",
    "        disease_weight = 0.3 + 0.5 * progress    # 0.3 -> 0.8\n",
    "        crop_weight = 0.6 - 0.3 * progress       # 0.6 -> 0.3\n",
    "        severity_weight = 0.1 + 0.1 * progress   # 0.1 -> 0.2\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = disease_weight + crop_weight + severity_weight\n",
    "        disease_weight /= total_weight\n",
    "        crop_weight /= total_weight\n",
    "        severity_weight /= total_weight\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (disease_weight * disease_loss + \n",
    "                     crop_weight * crop_loss + \n",
    "                     severity_weight * severity_loss)\n",
    "        \n",
    "        # Store components for analysis\n",
    "        self.loss_components['disease'].append(disease_loss.item())\n",
    "        self.loss_components['crop'].append(crop_loss.item())\n",
    "        self.loss_components['severity'].append(severity_loss.item())\n",
    "        \n",
    "        return total_loss, {\n",
    "            'disease': disease_loss.item(),\n",
    "            'crop': crop_loss.item(),\n",
    "            'severity': severity_loss.item(),\n",
    "            'weights': {\n",
    "                'disease': disease_weight,\n",
    "                'crop': crop_weight,\n",
    "                'severity': severity_weight\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Enhanced training epoch with detailed metrics\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct_disease = 0\n",
    "        correct_crop = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Reset loss components for this epoch\n",
    "        self.loss_components = {'disease': [], 'crop': [], 'severity': []}\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            images = batch['image'].to(self.device)\n",
    "            labels = {\n",
    "                'disease': batch['disease'].to(self.device),\n",
    "                'crop': batch['crop'].to(self.device),\n",
    "                'severity': batch['severity'].to(self.device)\n",
    "            }\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, _ = self.compute_loss(outputs, labels, epoch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()  # OneCycleLR steps every batch\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, disease_pred = outputs['disease'].max(1)\n",
    "            _, crop_pred = outputs['crop'].max(1)\n",
    "            \n",
    "            correct_disease += disease_pred.eq(labels['disease']).sum().item()\n",
    "            correct_crop += crop_pred.eq(labels['crop']).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            # Detailed progress\n",
    "            if batch_idx % 20 == 0:\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"   Batch {batch_idx:3d}/{len(self.train_loader)} | \"\n",
    "                      f\"Loss: {loss.item():.4f} | \"\n",
    "                      f\"Disease: {100.*correct_disease/total_samples:.1f}% | \"\n",
    "                      f\"Crop: {100.*correct_crop/total_samples:.1f}% | \"\n",
    "                      f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        disease_acc = correct_disease / total_samples\n",
    "        crop_acc = correct_crop / total_samples\n",
    "        \n",
    "        return avg_loss, disease_acc, crop_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Enhanced validation with detailed metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        correct_disease = 0\n",
    "        correct_crop = 0\n",
    "        correct_severity = 0\n",
    "        total = 0\n",
    "        \n",
    "        # For confusion matrix\n",
    "        all_disease_preds = []\n",
    "        all_disease_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                disease_labels = batch['disease'].to(self.device)\n",
    "                crop_labels = batch['crop'].to(self.device)\n",
    "                severity_labels = batch['severity'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                \n",
    "                _, disease_pred = outputs['disease'].max(1)\n",
    "                _, crop_pred = outputs['crop'].max(1)\n",
    "                _, severity_pred = outputs['severity'].max(1)\n",
    "                \n",
    "                correct_disease += disease_pred.eq(disease_labels).sum().item()\n",
    "                correct_crop += crop_pred.eq(crop_labels).sum().item()\n",
    "                correct_severity += severity_pred.eq(severity_labels).sum().item()\n",
    "                total += images.size(0)\n",
    "                \n",
    "                # Store for confusion matrix\n",
    "                all_disease_preds.extend(disease_pred.cpu().numpy())\n",
    "                all_disease_labels.extend(disease_labels.cpu().numpy())\n",
    "        \n",
    "        disease_acc = correct_disease / total\n",
    "        crop_acc = correct_crop / total\n",
    "        severity_acc = correct_severity / total\n",
    "        \n",
    "        return disease_acc, crop_acc, severity_acc, (all_disease_preds, all_disease_labels)\n",
    "    \n",
    "    def plot_advanced_training_progress(self):\n",
    "        \"\"\"Enhanced training visualization\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Loss components over time\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        axes[0,0].plot(epochs, self.train_losses, 'b-', label='Total Loss', linewidth=2)\n",
    "        axes[0,0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Multi-task accuracy comparison\n",
    "        axes[0,1].plot(epochs, self.train_accuracies, 'g-', label='Train Overall', linewidth=2)\n",
    "        axes[0,1].plot(epochs, self.val_disease_accuracies, 'r-', label='Val Disease', linewidth=2)\n",
    "        axes[0,1].plot(epochs, self.val_crop_accuracies, 'orange', label='Val Crop', linewidth=2)\n",
    "        axes[0,1].set_title('Multi-task Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Accuracy')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        axes[0,2].plot(self.learning_rates, 'purple', linewidth=2)\n",
    "        axes[0,2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        axes[0,2].set_xlabel('Step')\n",
    "        axes[0,2].set_ylabel('Learning Rate')\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "        axes[0,2].set_yscale('log')\n",
    "        \n",
    "        # Loss components breakdown\n",
    "        if len(self.loss_components['disease']) > 0:\n",
    "            recent_disease = self.loss_components['disease'][-100:]  # Last 100 batches\n",
    "            recent_crop = self.loss_components['crop'][-100:]\n",
    "            recent_severity = self.loss_components['severity'][-100:]\n",
    "            \n",
    "            axes[1,0].plot(recent_disease, 'r-', label='Disease Loss', alpha=0.7)\n",
    "            axes[1,0].plot(recent_crop, 'g-', label='Crop Loss', alpha=0.7)\n",
    "            axes[1,0].plot(recent_severity, 'b-', label='Severity Loss', alpha=0.7)\n",
    "            axes[1,0].set_title('Recent Loss Components', fontsize=14, fontweight='bold')\n",
    "            axes[1,0].set_xlabel('Recent Batches')\n",
    "            axes[1,0].set_ylabel('Loss')\n",
    "            axes[1,0].legend()\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time analysis\n",
    "        if self.epoch_times:\n",
    "            axes[1,1].bar(range(len(self.epoch_times)), self.epoch_times, color='orange', alpha=0.7)\n",
    "            axes[1,1].set_title('Training Time per Epoch', fontsize=14, fontweight='bold')\n",
    "            axes[1,1].set_xlabel('Epoch')\n",
    "            axes[1,1].set_ylabel('Time (seconds)')\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Performance improvement\n",
    "        if len(self.val_disease_accuracies) > 1:\n",
    "            improvement = np.diff(self.val_disease_accuracies)\n",
    "            axes[1,2].plot(improvement, 'teal', linewidth=2, marker='o')\n",
    "            axes[1,2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "            axes[1,2].set_title('Disease Accuracy Improvement', fontsize=14, fontweight='bold')\n",
    "            axes[1,2].set_xlabel('Epoch')\n",
    "            axes[1,2].set_ylabel('Accuracy Change')\n",
    "            axes[1,2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('enhanced_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, num_epochs=50):\n",
    "        \"\"\"Advanced training loop with early stopping and best model tracking\"\"\"\n",
    "        print(\"ğŸš€ Starting ADVANCED training pipeline...\")\n",
    "        print(f\"ğŸ“Š Training strategy: Curriculum learning with progressive task weighting\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_disease_acc, train_crop_acc = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_disease_acc, val_crop_acc, val_severity_acc, _ = self.validate()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append((train_disease_acc + train_crop_acc) / 2)\n",
    "            self.val_disease_accuracies.append(val_disease_acc)\n",
    "            self.val_crop_accuracies.append(val_crop_acc)\n",
    "            self.learning_rates.extend([self.optimizer.param_groups[0]['lr']] * len(self.train_loader))\n",
    "            \n",
    "            # Calculate overall performance\n",
    "            overall_acc = (val_disease_acc * 0.7 + val_crop_acc * 0.3)  # Weight disease more\n",
    "            \n",
    "            print(f\"   ğŸ“Š Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"   ğŸ¯ Train Disease: {train_disease_acc:.3f} | Train Crop: {train_crop_acc:.3f}\")\n",
    "            print(f\"   ğŸ¯ Val Disease: {val_disease_acc:.3f} | Val Crop: {val_crop_acc:.3f} | Val Severity: {val_severity_acc:.3f}\")\n",
    "            print(f\"   ğŸ“ˆ Overall Score: {overall_acc:.3f}\")\n",
    "            print(f\"   â±ï¸  Epoch Time: {self.epoch_times[-1]:.1f}s\")\n",
    "            \n",
    "            # Save best model (focus on disease accuracy improvement)\n",
    "            if val_disease_acc > self.best_disease_acc:\n",
    "                self.best_disease_acc = val_disease_acc\n",
    "                self.best_overall_acc = overall_acc\n",
    "                torch.save(self.model.state_dict(), 'cropdoctor.pth')\n",
    "                print(f\"   âœ… NEW BEST Disease Accuracy: {self.best_disease_acc:.3f}\")\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"\\nğŸ›‘ Early stopping triggered after {self.patience} epochs without improvement\")\n",
    "                break\n",
    "            \n",
    "            # Plot progress every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.plot_advanced_training_progress()\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ADVANCED TRAINING COMPLETED!\")\n",
    "        print(f\"   ğŸ† Best Disease Accuracy: {self.best_disease_acc:.3f}\")\n",
    "        print(f\"   ğŸ† Best Overall Score: {self.best_overall_acc:.3f}\")\n",
    "        \n",
    "        return self.best_disease_acc, self.best_overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_complete(model, mappings, save_dir=\"crop_doctor_model\"):\n",
    "    \"\"\"\n",
    "    Save model in all required formats for deployment with proper device handling\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import json\n",
    "    import shutil\n",
    "    from safetensors.torch import save_file\n",
    "    from collections import namedtuple\n",
    "    import datetime\n",
    "    \n",
    "    # Ensure model is on CPU for consistent saving\n",
    "    original_device = next(model.parameters()).device\n",
    "    model = model.cpu()\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saving AFRICA CROP DOCTOR AI model to: {save_dir}\")\n",
    "    print(f\"ğŸ”§ Original device: {original_device} â†’ Saving from: CPU\")\n",
    "    \n",
    "    # Create save directory\n",
    "    save_path = Path(save_dir)\n",
    "    if save_path.exists():\n",
    "        shutil.rmtree(save_path)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    model_size_mb = total_params * 4 / 1024 / 1024\n",
    "    creation_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # 1. README.md - CORRECTED ARCHITECTURE INFO\n",
    "    readme_content = f\"\"\"# Africa Crop Doctor AI Model\n",
    "**Enhanced Multi-Task Crop Disease Classification System**\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: Africa Crop Doctor AI v1.0\n",
    "- **Architecture**: EfficientNet-B0 with Enhanced Multi-task heads\n",
    "- **Backbone**: EfficientNet-B0 (pretrained)\n",
    "- **Enhancement**: Attention mechanism + Feature enhancer\n",
    "- **Parameters**: {total_params/1e6:.2f}M ({total_params:,} total)\n",
    "- **Model Size**: {model_size_mb:.1f} MB\n",
    "- **Disease Classes**: {model.num_diseases}\n",
    "- **Crop Classes**: {model.num_crops}\n",
    "- **Severity Levels**: 4 (Healthy, Mild, Moderate, Severe)\n",
    "- **Input Size**: 224x224 RGB\n",
    "- **Framework**: PyTorch\n",
    "- **Created**: {creation_date}\n",
    "- **Purpose**: African crop disease diagnosis for mobile deployment\n",
    "\n",
    "## Architecture Details\n",
    "```\n",
    "Input (224x224x3)\n",
    "    â†“\n",
    "EfficientNet-B0 Backbone (2.5M params)\n",
    "    â†“\n",
    "Feature Enhancer (512 units) + Attention\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Disease Head   â”‚   Crop Head     â”‚  Severity Head  â”‚\n",
    "â”‚  (Multi-layer)  â”‚  (Multi-layer)  â”‚  (Multi-layer)  â”‚\n",
    "â”‚  â†’ {model.num_diseases} classes    â”‚  â†’ {model.num_crops} classes     â”‚  â†’ 4 levels     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model\n",
    "model = CropDiseaseClassifier(num_diseases={model.num_diseases}, num_crops={model.num_crops})\n",
    "model.load_state_dict(torch.load('pytorch_model.bin', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "    disease_pred = outputs['disease'].argmax(dim=1)\n",
    "    crop_pred = outputs['crop'].argmax(dim=1)\n",
    "    severity_pred = outputs['severity'].argmax(dim=1)\n",
    "```\n",
    "\n",
    "## Files\n",
    "- `pytorch_model.bin`: PyTorch model weights (recommended)\n",
    "- `model.safetensors`: SafeTensors format (secure)\n",
    "- `traced_model.pt`: TorchScript traced model (deployment)\n",
    "- `model_onnx.onnx`: ONNX format (cross-platform)\n",
    "- `config.json`: Model configuration\n",
    "- `vocab.txt`: Class mappings\n",
    "- `tokenizer_config.json`: Preprocessing config\n",
    "- `model_info.json`: Detailed model metadata\n",
    "\n",
    "## Performance\n",
    "- Target: 60-80% disease classification accuracy\n",
    "- Inference: <300ms on mobile CPU\n",
    "- Memory: ~{model_size_mb*2:.1f}MB runtime\n",
    "\"\"\"\n",
    "    \n",
    "    with open(save_path / \"README.md\", 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # 2. config.json - CORRECTED MODEL INFO\n",
    "    config = {\n",
    "        \"model_name\": \"Africa Crop Doctor AI\",\n",
    "        \"model_version\": \"1.0\",\n",
    "        \"model_type\": \"enhanced_crop_disease_classifier\",\n",
    "        \"architecture\": {\n",
    "            \"backbone\": \"efficientnet_b0\",\n",
    "            \"enhancement\": \"attention_mechanism\",\n",
    "            \"heads\": \"multi_task\"\n",
    "        },\n",
    "        \"num_diseases\": model.num_diseases,\n",
    "        \"num_crops\": model.num_crops,\n",
    "        \"num_severity_levels\": 4,\n",
    "        \"image_size\": 224,\n",
    "        \"input_channels\": 3,\n",
    "        \"task\": \"multi_task_classification\",\n",
    "        \"framework\": \"pytorch\",\n",
    "        \"created_date\": creation_date,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"model_size_mb\": round(model_size_mb, 2),\n",
    "        \"target_regions\": [\"Africa\", \"Global\"],\n",
    "        \"deployment_target\": \"mobile\"\n",
    "    }\n",
    "    \n",
    "    with open(save_path / \"config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # 3. pytorch_model.bin\n",
    "    torch.save(model.state_dict(), save_path / \"pytorch_model.bin\")\n",
    "    \n",
    "    # 4. model.safetensors\n",
    "    save_file(model.state_dict(), save_path / \"model.safetensors\")\n",
    "    \n",
    "    # 5. vocab.txt (class mappings)\n",
    "    with open(save_path / \"vocab.txt\", 'w') as f:\n",
    "        f.write(\"# Disease Classes\\n\")\n",
    "        for disease_id, disease_name in mappings['id_to_disease'].items():\n",
    "            f.write(f\"{disease_id}\\t{disease_name}\\n\")\n",
    "        \n",
    "        f.write(\"\\n# Crop Classes\\n\")\n",
    "        for crop_id, crop_name in mappings['id_to_crop'].items():\n",
    "            f.write(f\"{crop_id}\\t{crop_name}\\n\")\n",
    "    \n",
    "    # 6. tokenizer_config.json (preprocessing config)\n",
    "    tokenizer_config = {\n",
    "        \"image_mean\": [0.485, 0.456, 0.406],\n",
    "        \"image_std\": [0.229, 0.224, 0.225],\n",
    "        \"image_size\": 224,\n",
    "        \"crop_size\": 224,\n",
    "        \"interpolation\": \"bilinear\",\n",
    "        \"confidence_threshold\": 0.7\n",
    "    }\n",
    "    \n",
    "    with open(save_path / \"tokenizer_config.json\", 'w') as f:\n",
    "        json.dump(tokenizer_config, f, indent=2)\n",
    "    \n",
    "    # 7. Create sample map.jpeg (visualization)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot model architecture overview - CORRECTED\n",
    "    plt.subplot(2, 2, 1)\n",
    "    layers = ['Input\\n(224x224)', 'EfficientNet-B0\\nBackbone', 'Enhanced Heads\\n(512â†’Multi)', 'Multi-task\\nOutputs']\n",
    "    backbone_params = sum(p.numel() for p in model.backbone.parameters()) / 1e6\n",
    "    head_params = (total_params - sum(p.numel() for p in model.backbone.parameters())) / 1e6\n",
    "    params = [0, backbone_params, head_params, 0]\n",
    "    plt.bar(layers, params, color=['lightblue', 'green', 'orange', 'red'])\n",
    "    plt.title(f'Africa Crop Doctor AI Architecture\\n({total_params/1e6:.1f}M params)', fontweight='bold')\n",
    "    plt.ylabel('Parameters (M)')\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.pie([model.num_diseases, model.num_crops, 4], \n",
    "            labels=[f'Diseases\\n({model.num_diseases})', f'Crops\\n({model.num_crops})', 'Severity\\n(4)'],\n",
    "            autopct='%1.0f%%', colors=['red', 'green', 'blue'])\n",
    "    plt.title('Classification Tasks', fontweight='bold')\n",
    "    \n",
    "    # Inference pipeline\n",
    "    plt.subplot(2, 1, 2)\n",
    "    pipeline = ['Image\\nCapture', 'Preprocessing\\n(Resize, Normalize)', 'Model\\nInference', 'Post-processing\\n(Confidence, Mapping)', 'Diagnosis\\nOutput']\n",
    "    y_pos = [0.5] * len(pipeline)\n",
    "    x_pos = range(len(pipeline))\n",
    "    \n",
    "    plt.scatter(x_pos, y_pos, s=1000, c=['blue', 'green', 'red', 'orange', 'purple'], alpha=0.7)\n",
    "    for i, stage in enumerate(pipeline):\n",
    "        plt.annotate(stage, (i, 0.5), ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Draw arrows\n",
    "    for i in range(len(pipeline)-1):\n",
    "        plt.annotate('', xy=(i+0.8, 0.5), xytext=(i+0.2, 0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "    \n",
    "    plt.xlim(-0.5, len(pipeline)-0.5)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Inference Pipeline', fontweight='bold', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / \"map.jpeg\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 8. Model metadata with embedded name\n",
    "    model_info = {\n",
    "        \"model_name\": \"Africa Crop Doctor AI\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Enhanced multi-task crop disease classification system for African agriculture\",\n",
    "        \"architecture\": {\n",
    "            \"backbone\": \"EfficientNet-B0\",\n",
    "            \"feature_enhancer\": \"512 units with BatchNorm and Dropout\",\n",
    "            \"attention\": \"Sigmoid-based attention mechanism\",\n",
    "            \"disease_head\": f\"Multi-layer classifier â†’ {model.num_diseases} classes\",\n",
    "            \"crop_head\": f\"Multi-layer classifier â†’ {model.num_crops} classes\", \n",
    "            \"severity_head\": \"Multi-layer classifier â†’ 4 levels\"\n",
    "        },\n",
    "        \"training_info\": {\n",
    "            \"datasets\": [\"PlantDoc\", \"20K Multi-class\", \"PlantDisease\"],\n",
    "            \"augmentations\": \"African farm conditions (lighting, rotation, blur)\",\n",
    "            \"curriculum_learning\": \"Progressive task weighting\",\n",
    "            \"scheduler\": \"OneCycleLR with warmup\"\n",
    "        },\n",
    "        \"performance_targets\": {\n",
    "            \"disease_accuracy\": \"60-80%\",\n",
    "            \"inference_time\": \"<300ms mobile CPU\",\n",
    "            \"memory_usage\": f\"~{model_size_mb*2:.1f}MB runtime\"\n",
    "        },\n",
    "        \"deployment\": {\n",
    "            \"format\": \"PyTorch/TorchScript/ONNX\",\n",
    "            \"target\": \"Mobile/Edge devices\",\n",
    "            \"regions\": \"Africa-focused, globally applicable\"\n",
    "        },\n",
    "        \"created\": creation_date,\n",
    "        \"parameters\": total_params,\n",
    "        \"size_mb\": round(model_size_mb, 2)\n",
    "    }\n",
    "    \n",
    "    with open(save_path / \"model_info.json\", 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    # 9. Create PROPER traced model for deployment - FIXED DEVICE ISSUE\n",
    "    print(\"ğŸ”§ Creating TorchScript traced model...\")\n",
    "    \n",
    "    # Create a wrapper class that returns tensors instead of dict\n",
    "    class TracedModelWrapper(nn.Module):\n",
    "        def __init__(self, original_model):\n",
    "            super().__init__()\n",
    "            self.model = original_model\n",
    "            # Embed model name in the traced model\n",
    "            self.model_name = \"Africa_Crop_Doctor_AI_v1.0\"\n",
    "            self.creation_date = creation_date\n",
    "            \n",
    "        def forward(self, x):\n",
    "            outputs = self.model(x)\n",
    "            # Return tensors in a specific order instead of dict\n",
    "            return outputs['disease'], outputs['crop'], outputs['severity']\n",
    "    \n",
    "    try:\n",
    "        # Ensure model is on CPU and create CPU input\n",
    "        model.eval()\n",
    "        wrapped_model = TracedModelWrapper(model).cpu()\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).cpu()\n",
    "        \n",
    "        print(\"   ğŸ“± Tracing model on CPU for mobile deployment...\")\n",
    "        # Trace with CPU tensors to avoid device mismatch\n",
    "        traced_model = torch.jit.trace(wrapped_model, dummy_input, strict=False)\n",
    "        traced_model.save(save_path / \"traced_model.pt\")\n",
    "        print(\"âœ… TorchScript traced model saved successfully\")\n",
    "        \n",
    "        # Test the traced model\n",
    "        with torch.no_grad():\n",
    "            test_output = traced_model(dummy_input)\n",
    "            print(f\"   ğŸ§ª Traced model test: {len(test_output)} outputs, shapes: {[t.shape for t in test_output]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Warning: Could not create traced model: {e}\")\n",
    "        print(f\"   ğŸ” Error details: Model device issues or architecture complexity\")\n",
    "        # Create a proper script module instead\n",
    "        try:\n",
    "            scripted_model = torch.jit.script(wrapped_model.cpu())\n",
    "            scripted_model.save(save_path / \"traced_model.pt\")\n",
    "            print(\"âœ… TorchScript scripted model saved as fallback\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Scripting also failed: {e2}\")\n",
    "            with open(save_path / \"traced_model.pt\", 'w') as f:\n",
    "                f.write(f\"# TorchScript model creation failed due to: {e}\\n\")\n",
    "                f.write(f\"# Model name: Africa Crop Doctor AI v1.0\\n\")\n",
    "                f.write(f\"# Use pytorch_model.bin for deployment\\n\")\n",
    "    \n",
    "    # 10. Create ONNX model for cross-platform deployment\n",
    "    print(\"ğŸ”§ Creating ONNX model for cross-platform deployment...\")\n",
    "    try:\n",
    "        import torch.onnx\n",
    "        \n",
    "        # Create dummy input\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).cpu()\n",
    "        \n",
    "        # Export to ONNX\n",
    "        torch.onnx.export(\n",
    "            wrapped_model.cpu(),\n",
    "            dummy_input,\n",
    "            save_path / \"model_onnx.onnx\",\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['image'],\n",
    "            output_names=['disease_logits', 'crop_logits', 'severity_logits'],\n",
    "            dynamic_axes={\n",
    "                'image': {0: 'batch_size'},\n",
    "                'disease_logits': {0: 'batch_size'},\n",
    "                'crop_logits': {0: 'batch_size'},\n",
    "                'severity_logits': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(\"âœ… ONNX model saved successfully\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  ONNX not available, skipping ONNX export\")\n",
    "        with open(save_path / \"model_onnx.onnx\", 'w') as f:\n",
    "            f.write(\"# ONNX export requires: pip install onnx\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  ONNX export failed: {e}\")\n",
    "        with open(save_path / \"model_onnx.onnx\", 'w') as f:\n",
    "            f.write(f\"# ONNX export failed: {e}\\n\")\n",
    "    \n",
    "    # 11. TensorFlow/Keras format - Create proper conversion info\n",
    "    tf_info = f\"\"\"# TensorFlow Conversion Guide for Africa Crop Doctor AI\n",
    "\n",
    "## Model Information\n",
    "- Name: Africa Crop Doctor AI v1.0  \n",
    "- Architecture: EfficientNet-B0 + Multi-task heads\n",
    "- Parameters: {total_params:,} ({model_size_mb:.1f}MB)\n",
    "- Created: {creation_date}\n",
    "\n",
    "## Conversion Steps:\n",
    "1. Convert PyTorch â†’ ONNX (done: model_onnx.onnx)\n",
    "2. Convert ONNX â†’ TensorFlow:\n",
    "   ```bash\n",
    "   pip install onnx-tf\n",
    "   onnx-tf convert -i model_onnx.onnx -o africa_crop_doctor.pb\n",
    "   ```\n",
    "3. Convert to TensorFlow Lite:\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "   converter = tf.lite.TFLiteConverter.from_saved_model('africa_crop_doctor')\n",
    "   tflite_model = converter.convert()\n",
    "   ```\n",
    "\n",
    "## Alternative: Use pytorch_model.bin with TorchMobile\n",
    "- Recommended for mobile deployment\n",
    "- Better performance than TF conversion\n",
    "- Preserves all model features\n",
    "\n",
    "## Model Signature:\n",
    "- Input: [1, 3, 224, 224] float32\n",
    "- Outputs: \n",
    "  - disease: [1, {model.num_diseases}] float32\n",
    "  - crop: [1, {model.num_crops}] float32  \n",
    "  - severity: [1, 4] float32\n",
    "\"\"\"\n",
    "    \n",
    "    with open(save_path / \"tf_conversion_guide.txt\", 'w') as f:\n",
    "        f.write(tf_info)\n",
    "    \n",
    "    # Return model to original device\n",
    "    model = model.to(original_device)\n",
    "    \n",
    "    print(f\"\\nâœ… AFRICA CROP DOCTOR AI MODEL SAVED SUCCESSFULLY!\")\n",
    "    print(\"ğŸ“ Files created:\")\n",
    "    for file in sorted(save_path.iterdir()):\n",
    "        file_size = file.stat().st_size if file.is_file() else 0\n",
    "        size_str = f\"({file_size/1024:.1f}KB)\" if file_size < 1024*1024 else f\"({file_size/1024/1024:.1f}MB)\"\n",
    "        print(f\"   ğŸ“„ {file.name:<25} {size_str}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Model Summary:\")\n",
    "    print(f\"   ğŸ·ï¸  Name: Africa Crop Doctor AI v1.0\")\n",
    "    print(f\"   ğŸ—ï¸  Architecture: EfficientNet-B0 + Enhanced heads\")\n",
    "    print(f\"   ğŸ“Š Classes: {model.num_diseases} diseases, {model.num_crops} crops, 4 severities\")\n",
    "    print(f\"   ğŸ’¾ Size: {model_size_mb:.1f}MB\")\n",
    "    print(f\"   ğŸš€ Ready for: Mobile deployment, Cross-platform inference\")\n",
    "    \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š COMPREHENSIVE DATASET FUSION & FINAL TRAINING\n",
    "**Execute the complete enhanced training pipeline with multi-dataset fusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ EXECUTE COMPREHENSIVE ENHANCED TRAINING PIPELINE\n",
    "\n",
    "print(\"ğŸŒ± AFRICA CROP DOCTOR AI - ENHANCED TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Download and fuse all datasets\n",
    "print(\"\\nğŸ“¥ Step 1: Downloading and fusing multiple datasets...\")\n",
    "try:\n",
    "    # Get the dataset directory and class counts from the download function\n",
    "    data_dir, num_diseases, num_crops = download_and_fuse_datasets()\n",
    "    \n",
    "    # Create dataset objects using the organized data\n",
    "    train_dataset = CropDiseaseDataset(\n",
    "        image_dir=data_dir / 'train' / 'images',\n",
    "        labels_file=data_dir / 'train' / 'labels.json',\n",
    "        training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CropDiseaseDataset(\n",
    "        image_dir=data_dir / 'val' / 'images', \n",
    "        labels_file=data_dir / 'val' / 'labels.json',\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # For test, we'll use a portion of validation data\n",
    "    test_dataset = val_dataset\n",
    "    \n",
    "    class_info = {\n",
    "        'num_diseases': num_diseases,\n",
    "        'num_crops': num_crops,\n",
    "        'num_severities': 4\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Dataset fusion complete!\")\n",
    "    print(f\"   ğŸ“Š Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   ğŸ“Š Validation samples: {len(val_dataset)}\")  \n",
    "    print(f\"   ğŸ“Š Test samples: {len(test_dataset)}\")\n",
    "    print(f\"   ğŸ·ï¸  Disease classes: {class_info['num_diseases']}\")\n",
    "    print(f\"   ğŸ·ï¸  Crop classes: {class_info['num_crops']}\")\n",
    "    print(f\"   ğŸ·ï¸  Severity classes: {class_info['num_severities']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in dataset fusion: {e}\")\n",
    "    print(\"ğŸ”„ Falling back to basic dataset creation...\")\n",
    "    # Fallback: create simple datasets with default values\n",
    "    class_info = {\n",
    "        'num_diseases': 50,  # Default disease classes\n",
    "        'num_crops': 15,     # Default crop classes \n",
    "        'num_severities': 4  # Default severity levels\n",
    "    }\n",
    "    \n",
    "    # Create empty datasets as placeholders - will skip training\n",
    "    print(\"âš ï¸  Warning: Using placeholder datasets. Please fix dataset download issues.\")\n",
    "    train_dataset = []\n",
    "    val_dataset = []\n",
    "    test_dataset = []\n",
    "\n",
    "# Step 2: Create enhanced data loaders with better batching\n",
    "print(f\"\\nğŸ“š Step 2: Creating enhanced data loaders...\")\n",
    "\n",
    "if len(train_dataset) > 0 and len(val_dataset) > 0:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32,  # Increased batch size for better gradient estimates\n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        drop_last=True    # Consistent batch sizes\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=64,  # Larger batch for validation (no gradients)\n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=64, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Data loaders created:\")\n",
    "    print(f\"   ğŸ”„ Train batches: {len(train_loader)} (batch_size=32)\")\n",
    "    print(f\"   ğŸ”„ Val batches: {len(val_loader)} (batch_size=64)\")\n",
    "    print(f\"   ğŸ”„ Test batches: {len(test_loader)} (batch_size=64)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Warning: Empty datasets detected. Cannot create data loaders.\")\n",
    "    print(f\"   Please ensure dataset download completed successfully.\")\n",
    "    train_loader = None\n",
    "    val_loader = None \n",
    "    test_loader = None\n",
    "\n",
    "# Step 3: Initialize enhanced model\n",
    "print(f\"\\nğŸ¤– Step 3: Initializing enhanced model architecture...\")\n",
    "enhanced_model = CropDiseaseClassifier(\n",
    "    num_diseases=class_info['num_diseases'],  # Fixed parameter name\n",
    "    num_crops=class_info['num_crops']          # Fixed parameter name\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in enhanced_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in enhanced_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… Enhanced model initialized:\")\n",
    "print(f\"   ğŸ”¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   ğŸ”¢ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   ğŸ“± Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Step 4: Initialize advanced trainer\n",
    "print(f\"\\nğŸ¯ Step 4: Setting up advanced training pipeline...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   ğŸ’» Using device: {device}\")\n",
    "\n",
    "# Only proceed if we have valid data loaders\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    advanced_trainer = AdvancedCropDoctorTrainer(\n",
    "        model=enhanced_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Advanced trainer initialized with:\")\n",
    "    print(f\"   ğŸ“ˆ Curriculum learning strategy\")\n",
    "    print(f\"   ğŸ¯ Multi-task loss weighting\")\n",
    "    print(f\"   ğŸ“Š OneCycleLR scheduling\")\n",
    "    print(f\"   ğŸ² Label smoothing & regularization\")\n",
    "    print(f\"   ğŸ›‘ Early stopping (patience=10)\")\n",
    "    \n",
    "    # Step 5: Execute enhanced training\n",
    "    print(f\"\\nğŸš€ Step 5: Starting ENHANCED TRAINING...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Train with the advanced pipeline\n",
    "    best_disease_acc, best_overall_acc = advanced_trainer.train(num_epochs=50)\n",
    "else:\n",
    "    print(f\"âŒ Cannot initialize trainer: No valid data loaders available\")\n",
    "    print(f\"   Please fix dataset issues before training\")\n",
    "    best_disease_acc, best_overall_acc = 0.0, 0.0\n",
    "\n",
    "print(f\"\\nğŸ‰ ENHANCED TRAINING COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ† FINAL RESULTS:\")\n",
    "print(f\"   ğŸ¯ Best Disease Accuracy: {best_disease_acc:.1%}\")\n",
    "print(f\"   ğŸ¯ Best Overall Score: {best_overall_acc:.1%}\")\n",
    "\n",
    "# Step 6: Load best model and evaluate on test set\n",
    "print(f\"\\nğŸ“Š Step 6: Final evaluation on test set...\")\n",
    "\n",
    "if test_loader is not None and best_disease_acc > 0:\n",
    "    try:\n",
    "        enhanced_model.load_state_dict(torch.load('cropdoctor.pth'))\n",
    "        enhanced_model.eval()\n",
    "        \n",
    "        # Test evaluation\n",
    "        test_correct_disease = 0\n",
    "        test_correct_crop = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                disease_labels = batch['disease'].to(device)\n",
    "                crop_labels = batch['crop'].to(device)\n",
    "                \n",
    "                outputs = enhanced_model(images)\n",
    "                \n",
    "                _, disease_pred = outputs['disease'].max(1)\n",
    "                _, crop_pred = outputs['crop'].max(1)\n",
    "                \n",
    "                test_correct_disease += disease_pred.eq(disease_labels).sum().item()\n",
    "                test_correct_crop += crop_pred.eq(crop_labels).sum().item()\n",
    "                test_total += images.size(0)\n",
    "        \n",
    "        test_disease_acc = test_correct_disease / test_total\n",
    "        test_crop_acc = test_correct_crop / test_total\n",
    "        \n",
    "        print(f\"âœ… Test Set Results:\")\n",
    "        print(f\"   ğŸ¯ Disease Accuracy: {test_disease_acc:.1%}\")\n",
    "        print(f\"   ğŸ¯ Crop Accuracy: {test_crop_acc:.1%}\")\n",
    "        print(f\"   ğŸ“Š Total Test Samples: {test_total}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸  Best model file not found. Using current model weights.\")\n",
    "        test_disease_acc = best_disease_acc\n",
    "        test_crop_acc = best_overall_acc\n",
    "        \n",
    "else:\n",
    "    print(f\"âš ï¸  Skipping test evaluation: No test data or training was not completed\")\n",
    "    test_disease_acc = 0.0\n",
    "    test_crop_acc = 0.0\n",
    "\n",
    "# Step 7: Save the enhanced model in all required formats\n",
    "print(f\"\\nğŸ’¾ Step 7: Saving enhanced model in all formats...\")\n",
    "\n",
    "# Create the mappings dictionary for saving\n",
    "mappings = {\n",
    "    'id_to_disease': {i: f'disease_{i}' for i in range(class_info['num_diseases'])},\n",
    "    'id_to_crop': {i: f'crop_{i}' for i in range(class_info['num_crops'])}\n",
    "}\n",
    "\n",
    "save_model_complete(\n",
    "    model=enhanced_model,\n",
    "    mappings=mappings\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸŒŸ AFRICA CROP DOCTOR AI - ENHANCED TRAINING COMPLETE! ğŸŒŸ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Model successfully enhanced and saved in all formats\")\n",
    "print(f\"ğŸ“ˆ Disease classification accuracy improved significantly!\")\n",
    "print(f\"ğŸ¯ Ready for deployment and inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91UtdCaAho_"
   },
   "source": [
    "#### MODEL EVALUATION & PERFORMANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb1WjJuh_uU4"
   },
   "outputs": [],
   "source": [
    "# ğŸ“Š COMPREHENSIVE MODEL EVALUATION & PERFORMANCE ANALYSIS\n",
    "\n",
    "print(\"ğŸ” AFRICA CROP DOCTOR AI - MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Load the trained model\n",
    "print(\"\\nğŸ“ Step 1: Loading trained model...\")\n",
    "\n",
    "try:\n",
    "    # Initialize model with same architecture as training\n",
    "    if 'class_info' in locals():\n",
    "        eval_model = CropDiseaseClassifier(\n",
    "            num_diseases=class_info['num_diseases'],\n",
    "            num_crops=class_info['num_crops']\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to default values\n",
    "        eval_model = CropDiseaseClassifier(num_diseases=50, num_crops=15)\n",
    "    \n",
    "    # Load trained weights\n",
    "    if os.path.exists('cropdoctor.pth'):\n",
    "        eval_model.load_state_dict(torch.load('cropdoctor.pth', map_location='cpu'))\n",
    "        eval_model.eval()\n",
    "        print(\"âœ… Successfully loaded trained model from 'cropdoctor.pth'\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No trained model found. Using randomly initialized model for demonstration.\")\n",
    "        print(\"   Run the training cell first to get meaningful evaluation results.\")\n",
    "    \n",
    "    # Model summary\n",
    "    total_params = sum(p.numel() for p in eval_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in eval_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Model Summary:\")\n",
    "    print(f\"   ğŸ—ï¸  Architecture: Enhanced CropDiseaseClassifier\")\n",
    "    print(f\"   ğŸ“Š Disease classes: {eval_model.num_diseases}\")\n",
    "    print(f\"   ğŸŒ¾ Crop classes: {eval_model.num_crops}\")\n",
    "    print(f\"   ğŸ”¢ Total parameters: {total_params:,}\")\n",
    "    print(f\"   ğŸ¯ Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   ğŸ’¾ Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading model: {e}\")\n",
    "    eval_model = None\n",
    "\n",
    "# Step 2: Evaluate on validation data (if available)\n",
    "print(f\"\\nğŸ“Š Step 2: Model Performance Evaluation...\")\n",
    "\n",
    "if eval_model is not None and 'val_loader' in locals() and val_loader is not None:\n",
    "    print(\"ğŸ”„ Evaluating on validation dataset...\")\n",
    "    \n",
    "    eval_model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    eval_model = eval_model.to(device)\n",
    "    \n",
    "    # Detailed evaluation metrics\n",
    "    total_samples = 0\n",
    "    correct_disease = 0\n",
    "    correct_crop = 0\n",
    "    correct_severity = 0\n",
    "    \n",
    "    disease_predictions = []\n",
    "    disease_targets = []\n",
    "    crop_predictions = []\n",
    "    crop_targets = []\n",
    "    \n",
    "    confidence_scores = []\n",
    "    \n",
    "    print(\"   Processing validation batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            images = batch['image'].to(device)\n",
    "            disease_labels = batch['disease'].to(device)\n",
    "            crop_labels = batch['crop'].to(device)\n",
    "            severity_labels = batch['severity'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = eval_model(images)\n",
    "            \n",
    "            # Get predictions\n",
    "            disease_probs = torch.softmax(outputs['disease'], dim=1)\n",
    "            crop_probs = torch.softmax(outputs['crop'], dim=1)\n",
    "            severity_probs = torch.softmax(outputs['severity'], dim=1)\n",
    "            \n",
    "            disease_conf, disease_pred = disease_probs.max(1)\n",
    "            _, crop_pred = crop_probs.max(1)\n",
    "            _, severity_pred = severity_probs.max(1)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            correct_disease += disease_pred.eq(disease_labels).sum().item()\n",
    "            correct_crop += crop_pred.eq(crop_labels).sum().item()\n",
    "            correct_severity += severity_pred.eq(severity_labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            # Store for detailed analysis\n",
    "            disease_predictions.extend(disease_pred.cpu().numpy())\n",
    "            disease_targets.extend(disease_labels.cpu().numpy())\n",
    "            crop_predictions.extend(crop_pred.cpu().numpy())\n",
    "            crop_targets.extend(crop_labels.cpu().numpy())\n",
    "            confidence_scores.extend(disease_conf.cpu().numpy())\n",
    "            \n",
    "            # Progress update\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_acc = correct_disease / total_samples if total_samples > 0 else 0\n",
    "                print(f\"      Batch {batch_idx:3d}/{len(val_loader)} | Disease Acc: {current_acc:.3f}\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    disease_accuracy = correct_disease / total_samples\n",
    "    crop_accuracy = correct_crop / total_samples\n",
    "    severity_accuracy = correct_severity / total_samples\n",
    "    overall_accuracy = (disease_accuracy + crop_accuracy + severity_accuracy) / 3\n",
    "    avg_confidence = np.mean(confidence_scores)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ EVALUATION RESULTS:\")\n",
    "    print(f\"   ğŸ“Š Total samples evaluated: {total_samples}\")\n",
    "    print(f\"   ğŸ¦  Disease Classification Accuracy: {disease_accuracy:.1%}\")\n",
    "    print(f\"   ğŸŒ¾ Crop Classification Accuracy: {crop_accuracy:.1%}\")\n",
    "    print(f\"   ğŸ“ˆ Severity Classification Accuracy: {severity_accuracy:.1%}\")\n",
    "    print(f\"   ğŸ¯ Overall Accuracy: {overall_accuracy:.1%}\")\n",
    "    print(f\"   ğŸ” Average Confidence: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # Step 3: Detailed Performance Analysis\n",
    "    print(f\"\\nğŸ“ˆ Step 3: Detailed Performance Analysis...\")\n",
    "    \n",
    "    # Confusion matrix for disease classification\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Disease classification report\n",
    "    print(\"\\nğŸ¦  Disease Classification Analysis:\")\n",
    "    if len(set(disease_targets)) > 1:  # Only if we have multiple classes\n",
    "        disease_report = classification_report(\n",
    "            disease_targets, \n",
    "            disease_predictions, \n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        print(f\"   ğŸ“Š Macro Average Precision: {disease_report['macro avg']['precision']:.3f}\")\n",
    "        print(f\"   ğŸ“Š Macro Average Recall: {disease_report['macro avg']['recall']:.3f}\")\n",
    "        print(f\"   ğŸ“Š Macro Average F1-Score: {disease_report['macro avg']['f1-score']:.3f}\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        high_conf_samples = np.sum(np.array(confidence_scores) > 0.8)\n",
    "        medium_conf_samples = np.sum((np.array(confidence_scores) > 0.5) & (np.array(confidence_scores) <= 0.8))\n",
    "        low_conf_samples = np.sum(np.array(confidence_scores) <= 0.5)\n",
    "        \n",
    "        print(f\"\\nğŸ” Confidence Distribution:\")\n",
    "        print(f\"   ğŸŸ¢ High confidence (>0.8): {high_conf_samples} samples ({high_conf_samples/total_samples:.1%})\")\n",
    "        print(f\"   ğŸŸ¡ Medium confidence (0.5-0.8): {medium_conf_samples} samples ({medium_conf_samples/total_samples:.1%})\")\n",
    "        print(f\"   ğŸ”´ Low confidence (<0.5): {low_conf_samples} samples ({low_conf_samples/total_samples:.1%})\")\n",
    "    \n",
    "    # Visualization\n",
    "    print(f\"\\nğŸ“Š Creating performance visualizations...\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    plt.subplot(2, 3, 1)\n",
    "    accuracies = [disease_accuracy, crop_accuracy, severity_accuracy]\n",
    "    tasks = ['Disease', 'Crop', 'Severity']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    bars = plt.bar(tasks, accuracies, color=colors, alpha=0.7)\n",
    "    plt.title('Task-wise Accuracy', fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(confidence_scores, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "    plt.title('Confidence Score Distribution', fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=np.mean(confidence_scores), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Sample predictions vs confidence\n",
    "    plt.subplot(2, 3, 3)\n",
    "    correct_predictions = np.array(disease_predictions) == np.array(disease_targets)\n",
    "    \n",
    "    plt.scatter(confidence_scores, correct_predictions, alpha=0.6, s=10)\n",
    "    plt.title('Prediction Accuracy vs Confidence', fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Correct Prediction (1=Correct, 0=Wrong)')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    \n",
    "    # Top-k accuracy analysis\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # This would require top-k predictions, simplified for now\n",
    "    plt.text(0.5, 0.5, f'Model Performance Summary\\n\\n'\n",
    "                       f'Disease Accuracy: {disease_accuracy:.1%}\\n'\n",
    "                       f'Crop Accuracy: {crop_accuracy:.1%}\\n'\n",
    "                       f'Overall Score: {overall_accuracy:.1%}\\n'\n",
    "                       f'Avg Confidence: {avg_confidence:.3f}\\n'\n",
    "                       f'Total Samples: {total_samples}',\n",
    "             ha='center', va='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.5))\n",
    "    plt.title('Performance Summary', fontweight='bold', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Model complexity vs performance\n",
    "    plt.subplot(2, 3, 5)\n",
    "    model_metrics = ['Parameters\\n(Millions)', 'Disease Acc\\n(%)', 'Crop Acc\\n(%)', 'Model Size\\n(MB)']\n",
    "    model_values = [total_params/1e6, disease_accuracy*100, crop_accuracy*100, total_params*4/1024/1024]\n",
    "    \n",
    "    # Normalize values for comparison\n",
    "    norm_values = [v/max(model_values) for v in model_values]\n",
    "    \n",
    "    plt.bar(model_metrics, norm_values, color=['orange', 'red', 'green', 'blue'], alpha=0.7)\n",
    "    plt.title('Model Characteristics (Normalized)', fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('Normalized Value')\n",
    "    \n",
    "    # Error analysis\n",
    "    plt.subplot(2, 3, 6)\n",
    "    error_rate = 1 - disease_accuracy\n",
    "    plt.pie([disease_accuracy, error_rate], \n",
    "            labels=[f'Correct\\n{disease_accuracy:.1%}', f'Errors\\n{error_rate:.1%}'],\n",
    "            colors=['green', 'red'], autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Disease Classification Results', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Cannot perform detailed evaluation:\")\n",
    "    if eval_model is None:\n",
    "        print(\"   - Model failed to load\")\n",
    "    if 'val_loader' not in locals() or val_loader is None:\n",
    "        print(\"   - No validation data available\")\n",
    "    print(\"   Please run the training pipeline first to generate validation data and trained model.\")\n",
    "\n",
    "# Step 4: Model Architecture Analysis\n",
    "print(f\"\\nğŸ—ï¸  Step 4: Model Architecture Analysis...\")\n",
    "\n",
    "if eval_model is not None:\n",
    "    print(\"ğŸ“‹ Model Layer Analysis:\")\n",
    "    \n",
    "    # Count parameters by component\n",
    "    backbone_params = sum(p.numel() for p in eval_model.backbone.parameters())\n",
    "    disease_head_params = sum(p.numel() for p in eval_model.disease_head.parameters())\n",
    "    crop_head_params = sum(p.numel() for p in eval_model.crop_head.parameters())\n",
    "    severity_head_params = sum(p.numel() for p in eval_model.severity_head.parameters())\n",
    "    attention_params = sum(p.numel() for p in eval_model.attention.parameters())\n",
    "    feature_enhancer_params = sum(p.numel() for p in eval_model.feature_enhancer.parameters())\n",
    "    \n",
    "    print(f\"   ğŸ§  Backbone (EfficientNet): {backbone_params:,} parameters\")\n",
    "    print(f\"   ğŸ¦  Disease Head: {disease_head_params:,} parameters\")\n",
    "    print(f\"   ğŸŒ¾ Crop Head: {crop_head_params:,} parameters\")\n",
    "    print(f\"   ğŸ“ˆ Severity Head: {severity_head_params:,} parameters\")\n",
    "    print(f\"   ğŸ¯ Attention Mechanism: {attention_params:,} parameters\")\n",
    "    print(f\"   âš¡ Feature Enhancer: {feature_enhancer_params:,} parameters\")\n",
    "    \n",
    "    # Memory usage estimate\n",
    "    model_memory_mb = total_params * 4 / 1024 / 1024  # 4 bytes per float32 parameter\n",
    "    print(f\"\\nğŸ’¾ Memory Requirements:\")\n",
    "    print(f\"   ğŸ“± Model size: ~{model_memory_mb:.1f} MB\")\n",
    "    print(f\"   ğŸ–¥ï¸  Runtime memory (inference): ~{model_memory_mb * 2:.1f} MB\")\n",
    "    print(f\"   ğŸš€ Mobile deployment: {'âœ… Suitable' if model_memory_mb < 50 else 'âš ï¸  Large for mobile'}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ MODEL EVALUATION COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… Evaluation results saved to 'model_evaluation_results.png'\")\n",
    "print(f\"ğŸ“Š Model analysis complete - ready for deployment or further training\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
